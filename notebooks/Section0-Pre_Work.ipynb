{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, import some of the libraries we will be using in the course. This will test that everything is installed correctly on your system. Running the cell below should not return errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "\n",
    "print('All packages imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was imported, you are ready to work through the exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Manipulation\n",
    "\n",
    "Loading and processing data has become easier since the development of the `pandas` library, which provides data structures and functions for automating key data operations. \n",
    "\n",
    "For importing data from most common storage formats, several `read_*` functions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read_clipboard',\n",
       " 'read_csv',\n",
       " 'read_excel',\n",
       " 'read_feather',\n",
       " 'read_fwf',\n",
       " 'read_gbq',\n",
       " 'read_hdf',\n",
       " 'read_html',\n",
       " 'read_json',\n",
       " 'read_msgpack',\n",
       " 'read_parquet',\n",
       " 'read_pickle',\n",
       " 'read_sas',\n",
       " 'read_spss',\n",
       " 'read_sql',\n",
       " 'read_sql_query',\n",
       " 'read_sql_table',\n",
       " 'read_stata',\n",
       " 'read_table']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f for f in dir(pd) if f.startswith('read_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `../data/` subdirectory includes some clinical trial data in `cdystonia.csv`. Choose the appropriate function and use it to import this data to a variable called `cdystonia`. Print the first 15 lines of the resulting `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the appropriate function to read `../data/cdystonia.csv` into a variable called `cdystonia`\n",
    "# Print the first 15 lines of the `DataFrame` `cdystonia`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is from [Statistical Methods for the Analysis of Repeated Measurements](http://www.amazon.com/Statistical-Methods-Analysis-Repeated-Measurements/dp/0387953701) by Charles S. Davis, pp. 161-163 (Springer, 2002). These data are from a multicenter, randomized controlled trial of botulinum toxin type B (BotB) in patients with cervical dystonia from nine U.S. sites.\n",
    "\n",
    "Patients were randomized to placebo (N=36), 5000 units of BotB (N=36), or 10,000 units of BotB (N=37). The response variable is the total score on Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS), measuring severity, pain, and disability of cervical dystonia (high scores mean more impairment). TWSTRS was measured at baseline (week 0) and weeks 2, 4, 8, 12, 16 after treatment began, so this is a longitudinal study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since there are repeated measures of each patient, the `patient` column alone cannot be used as an index, because it is not unique. Use some of the columns in the DataFrame to create an index for the data that is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a column in addition to \"patient\" so that the two columns have unique values, and set those two to be the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The `cdystonia` dataset is stored in **long** format, meaning that each row contains a single observation. Use pandas functions and methods to change the data to **wide** format, where each row represents the data for a single patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to a wide format, where each row represents the data for a single patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To get an idea about the efficacy of the treatment, use `pandas` to group the data by treatment group, and calculate the mean and standard deviation of the `twstrs` outcome variable for each group in week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by treatment group, and calculate the mean and standard deviation of the `twstrs` outcome variable for each group in week 4.\n",
    "# As a sanity check, this should be the answer here:\n",
    "\n",
    "#                mean       std\n",
    "# treat\n",
    "# 10000U         34.805556  12.188565\n",
    "# 5000U          37.114286  15.311993\n",
    "# Placebo        39.342857  11.827045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Since this is a longitudinal study, graphics are helpful for understanding the dynamics of the experiment. Using the plotting package of your choice (there are many for Python!) create a set of plots showing how the response variable changes over time for each experimental group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of plots showing how the response variable changes over time for each experimental group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Operations become much faster when we can express them as vectorized linear algebra commands. One example is generating multivariate normal distributions.\n",
    "\n",
    "Suppose we wish to generate samples with mean $\\mu$ and covariance $\\Sigma$, where $\\mu$ is $n \\times 1$ and $\\Sigma$ is $n \\times n$. One way to do this is to calculate a [*Cholesky decomposition*](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $\\Sigma$, so that\n",
    "$$\n",
    "\\Sigma = LL^T.\n",
    "$$\n",
    "\n",
    "Then if $x$ is $n$ independent draws from a standard normal distribution, \n",
    "$$\n",
    "\\nu = Lx + \\mu \\sim \\mathcal{N}(\\mu, \\Sigma)\n",
    "$$\n",
    "\n",
    "### Exercises\n",
    "Let $\\mu = (3, 2)$, and\n",
    "$$\n",
    "\\Sigma = \\left(\\begin{array}{cc}\n",
    "1 & 0.9 \\\\\n",
    "0.9 & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "1. Use `np.linalg.cholesky` to compute $L$, the Cholesky decomposition of $\\Sigma$\n",
    "2. In newer Python (3.6+), `@` is matrix multiplication. Confirm that $\\Sigma = LL^T$.\n",
    "3. Draw 2 independent draws from a standard normal using `x = np.random.randn(2)`, and compute $\\nu$ using `L @ x + mu`\n",
    "4. The above exercise generates 1 draw from a multivariate normal. Use `x = np.random.randn(2, 1_000)` and the same formula as above to generate 1,000 draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([[3], [2]])\n",
    "sigma = np.array([[1, 0.9], [0.9, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `np.linalg.cholesky` to compute $L$, the Cholesky decomposition of `sigma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In newer Python (3.6+), `@` is matrix multiplication. Confirm that `sigma = L @ L.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 2 independent draws from a standard normal using `x = np.random.randn(2)`, and compute `nu` using `L @ x + mu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above exercise generates 1 draw from a multivariate normal. Use `x = np.random.randn(2, 1_000)` and the same formula as above to generate 1,000 draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions and Simulation\n",
    "\n",
    "Bayesian inference relies on the use of probability distributions for constructing models. Though several statistical and machine learning packages implement their own set of probability distributions, the NumPy and SciPy libraries include general-purpose functions and classes for performing probability operations. \n",
    "\n",
    "NumPy has an efficient set of random number generators for different distributions, while SciPy implements a large set of complete probability distributions that allow them to be used in most applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import distributions as dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['truncnorm_gen',\n",
       " 'tukeylambda',\n",
       " 'tukeylambda_gen',\n",
       " 'uniform',\n",
       " 'uniform_gen',\n",
       " 'vonmises',\n",
       " 'vonmises_gen',\n",
       " 'vonmises_line',\n",
       " 'wald',\n",
       " 'wald_gen',\n",
       " 'weibull_max',\n",
       " 'weibull_max_gen',\n",
       " 'weibull_min',\n",
       " 'weibull_min_gen',\n",
       " 'wrapcauchy',\n",
       " 'wrapcauchy_gen',\n",
       " 'yulesimon',\n",
       " 'yulesimon_gen',\n",
       " 'zipf',\n",
       " 'zipf_gen']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dists)[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a gamma distribution as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_dist = dists.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the attributes of `gamma_dist`, we can see several important methods, including `pdf` (probability distribution function), `cdf` (cumulative distribution function), `rvs` (random number generator), and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'badvalue',\n",
       " 'cdf',\n",
       " 'entropy',\n",
       " 'expect',\n",
       " 'extradoc',\n",
       " 'fit',\n",
       " 'fit_loc_scale',\n",
       " 'freeze',\n",
       " 'generic_moment',\n",
       " 'interval',\n",
       " 'isf',\n",
       " 'logcdf',\n",
       " 'logpdf',\n",
       " 'logsf',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'moment',\n",
       " 'moment_type',\n",
       " 'name',\n",
       " 'nnlf',\n",
       " 'numargs',\n",
       " 'pdf',\n",
       " 'ppf',\n",
       " 'random_state',\n",
       " 'rvs',\n",
       " 'sf',\n",
       " 'shapes',\n",
       " 'stats',\n",
       " 'std',\n",
       " 'support',\n",
       " 'var',\n",
       " 'vecentropy',\n",
       " 'xtol']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f for f in dir(gamma_dist) if not f.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Simulating linear regression\n",
    "\n",
    "1. Generate a (1000, 10) `features` array using a uniform distribution.\n",
    "2. Generate a (10, 1) `weights` array, using a normal distribution with standard deviation 2 and mean 0. Note this is typically unobserved.\n",
    "3. Generate a (1000, 1) `noise` array, using a normal distribution with standard deviation 1 and mean 0. Note that this is typically unobserved.\n",
    "4. Compute (1000, 1) `target` array, as `features @ weights + noise`\n",
    "5. Recover an estimate for the weights using `np.linalg.pinv(features) @ target` (`pinv` is the [Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse))\n",
    "6. Our statistical model is \n",
    "$$\n",
    "\\text{weights} \\sim \\mathcal{N}(0, 2) \\\\\n",
    "\\text{target} | \\text{features}, \\text{weights} \\sim \\mathcal{N}(\\text{features} \\cdot \\text{weights}, 1)\n",
    "$$\n",
    "What is the log probability that all the weights are 0, and all the targets are 1, given your generated `features`?\n",
    "7. What is the log probability of your generated `weights` and generated `targets`, given your generated `features`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a (1000, 10) `features` array using a uniform distribution.\n",
    "\n",
    "# assert features.shape == (1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a (10, 1) `weights` array, using a normal distribution with standard deviation 2 and mean 0\n",
    "\n",
    "# assert weights.shape == (10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a (1000, 1) `noise` array, using a normal distribution with standard deviation 1 and mean 0\n",
    "\n",
    "# assert noise.shape == (1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute (1000, 1) `target` array, as `features @ weights + noise`\n",
    "\n",
    "# assert target.shape == (1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover an estimate for the weights using `np.linalg.pinv(features) @ target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log probability that all the weights are 0, and all the targets are 1, given your generated `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log probability of your generated `weights` and `targets`, given your generated `features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer to 5 above should match the answer `scikit-learn` provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# LinearRegression(fit_intercept=False).fit(features, target).coef_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of running linear regression is via optimization. Suppose we want to minimize the sum of squares using scipy. We can do this using `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "features = np.random.uniform(size=(1000, 10))\n",
    "weights = np.random.normal(0, 2, size=(10, 1))\n",
    "noise = np.random.normal(0, 1, size=(1000, 1))\n",
    "target = (features @ weights + noise).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Optimization\n",
    "\n",
    "Find the weights with the least mean squared error using `fmin_bfgs`. It expects a function to minimize and an initial point.\n",
    "\n",
    "1. The function to minimize should accept an argument `x`, and return `((features @ x - target) ** 2).mean()`\n",
    "2. You can initialize with a vector of 10 zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the function to minimize, and an initial point\n",
    "\n",
    "def loss_function(x):\n",
    "    pass\n",
    "\n",
    "# initial_point = \n",
    "\n",
    "# assert initial_point.shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_x = fmin_bfgs(loss_function, initial_point)\n",
    "# min_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer should match our solution using linear algebra, or using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.67850443, -1.25680759,  1.19892705, -2.75545697, -4.1307904 ,\n",
       "        1.85208691,  4.16179858,  1.93712451, -2.85141983, -0.37927617])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(features) @ target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.67850443, -1.25680759,  1.19892705, -2.75545697, -4.1307904 ,\n",
       "        1.85208691,  4.16179858,  1.93712451, -2.85141983, -0.37927617])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LinearRegression(fit_intercept=False).fit(features, target).coef_.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
